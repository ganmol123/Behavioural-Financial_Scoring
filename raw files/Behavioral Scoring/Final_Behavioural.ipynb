{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "import io \n",
    "import unicodedata \n",
    "import numpy as np \n",
    "import re \n",
    "import string \n",
    "from numpy import linalg \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "from nltk.corpus import webtext \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import requests \n",
    "import os, sys\n",
    "import json\n",
    "from nltk import tokenize\n",
    "\n",
    "def funcc(sentences):\n",
    "    res = {}\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        for k in sorted(ss):\n",
    "            res[k] = ss[k]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is to fetch Feed data from Facebook Graph API and store it to feed_data.json and feed_data.csv\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def Senti_Extractor(num):\n",
    "    if num>0:\n",
    "        return math.ceil(num)\n",
    "    else:\n",
    "        return math.floor(num)\n",
    "\n",
    "def Behavoural_analysis(s):\n",
    "    with urlopen(\n",
    "     \"https://graph.facebook.com/v6.0/me?fields=id%2Cname%2Cfeed%7Bcreated_time%2Cdescription%7D%2Cposts%7Bcreated_time%2Cmessage%2Cdescription%2Ccaption%7D&access_token={}\".format(s)) as response:\n",
    "        source = response.read()\n",
    "\n",
    "    data = json.loads(source)\n",
    "\n",
    "    some_data = data['feed']\n",
    "\n",
    "    outfile = open(\"feed_data.csv\", \"w\")\n",
    "\n",
    "    fieldNames = ['created_time', 'id', 'description']\n",
    "\n",
    "    writer = csv.DictWriter(outfile, fieldnames = fieldNames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    while len(some_data['data']) != 0:\n",
    "        with open('feed_data.json', 'a+', encoding='utf-8') as f:\n",
    "            json.dump(some_data['data'], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        for row in some_data['data']:\n",
    "            writer.writerow(row)\n",
    "    #         print(row)\n",
    "\n",
    "        paging_next = some_data['paging']['next']\n",
    "\n",
    "        with urlopen(paging_next) as response:\n",
    "            source = response.read()\n",
    "        some_data = json.loads(source)\n",
    "        \n",
    "    senti_value = pd.Series([])\n",
    "    confidence = pd.Series([])\n",
    "    \n",
    "    df = pd.read_csv(\"feed_data.csv\")\n",
    "    df = df[pd.notnull(df['description'])]\n",
    "    df['Response'] = np.nan\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(\"feed_clean.csv\")#, index = False)\n",
    "    \n",
    "    for i in range(len(df)):  # Change head() \n",
    "        temp = []\n",
    "        column = df['description'][i]\n",
    "        temp.append(column)\n",
    "#         print(funcc(column))\n",
    "        #print(column)\n",
    "        senti_dict = funcc(temp)\n",
    "        senti_value[i] = Senti_Extractor(senti_dict.get('compound'))\n",
    "        confidence[i] = senti_dict.get('compound')\n",
    "        #print('\\n')\n",
    "        \n",
    "    # Inserting Senti_Value and Confidence column\n",
    "\n",
    "    df.insert(4, \"Senti_Value\", senti_value)\n",
    "    df.insert(5, \"Confidence\", confidence)\n",
    "    df.drop(columns = ['Response'], inplace = True)\n",
    "    #return df.head()\n",
    "    \n",
    "    # Assigning weightage to DataTime\n",
    "\n",
    "    total_rows = len(df.index)\n",
    "    # total_rows = 21\n",
    "    even = False\n",
    "\n",
    "    time_weigh = pd.Series([])\n",
    "\n",
    "    if total_rows % 2 == 0:\n",
    "        even = True\n",
    "\n",
    "    if even == True:\n",
    "        i = total_rows/2 - 1\n",
    "        time_weigh[i] = 25\n",
    "        i = i-1\n",
    "        while(i >= 0):\n",
    "            time_weigh[i] = time_weigh[i+1]/2\n",
    "            i = i-1\n",
    "\n",
    "        i = total_rows/2\n",
    "        time_weigh[i] = 25\n",
    "        i = i+1\n",
    "        while(i < total_rows):\n",
    "            time_weigh[i] = time_weigh[i-1]/2\n",
    "            i = i+1\n",
    "    else:\n",
    "        i = total_rows/2 - 0.5\n",
    "        time_weigh[i] = 50/3\n",
    "        time_weigh[i+1] = 50/3\n",
    "        time_weigh[i-1] = 50/3\n",
    "        time_weigh[i-2] = 25/2\n",
    "        i = i - 3\n",
    "        while (i >= 0):\n",
    "            time_weigh[i] = time_weigh[i+1]/2\n",
    "            i = i - 1\n",
    "\n",
    "        i = total_rows/2 - 0.5\n",
    "        time_weigh[i+2] = 25/2\n",
    "        i = i+3\n",
    "        while (i < total_rows):\n",
    "            time_weigh[i] = time_weigh[i-1]/2\n",
    "            i = i+1\n",
    "\n",
    "    # print(time_weigh)\n",
    "    \n",
    "    # Inserting Time_Weight\n",
    "\n",
    "    df.insert(2, \"Time_Weight\", time_weigh)\n",
    "    #return df.head()\n",
    "    \n",
    "    # Ranking based on Confidence\n",
    "\n",
    "    df['Confi_Rank'] = df['Confidence'].rank(ascending = 0, method = 'dense')\n",
    "    # df.sort_values('Confi_Rank', inplace = True)\n",
    "    df = df.set_index('Confi_Rank')\n",
    "    df = df.sort_index()\n",
    "  \n",
    "    \n",
    "        # Assigning weightage based on Confidence\n",
    "\n",
    "    confi_weigh = pd.Series([])\n",
    "\n",
    "    confi_weigh[1] = 50\n",
    "\n",
    "    for ind in df.index:\n",
    "        if (ind != 1.0):\n",
    "            confi_weigh[ind] = confi_weigh[ind-1]/2\n",
    "    \n",
    "    df.insert(6, \"Confi_Weight\", confi_weigh)\n",
    "    \n",
    "    # Accumulating overall score for each Feed post\n",
    "    df['Overall'] = df['Time_Weight'] * df['Senti_Value'] * df['Confi_Weight']\n",
    "    \n",
    "    # Saving the final table\n",
    "    df.to_csv('feed_complete.csv')\n",
    "    \n",
    "    feed_score = df['Overall'].sum()\n",
    "    return feed_score\n",
    "    #return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6725248843863175\n"
     ]
    }
   ],
   "source": [
    "user_short_token = 'EAAYCDwuzUQ4BAMRp74iAtulphTizIY3L5WzDqCynH3QkQPWbDsihkssinqAipVm9anZBeVPpngUZBb1WOT0l9vCyFdPYaVVaCgPz66LTPI7ZCjo2Pj0G9X1efsAhcqNMz1avqyVAXeGgI0EhVZBKPKeF6OsqZBsFQQlvypozZCN92IQqDarfpdwOZB4ZCIpKdlGf5OOTa5m2AwZDZD'\n",
    "print(Behavoural_analysis(user_short_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post\n",
    "import json\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def Senti_Extractor(num):\n",
    "    if num>0:\n",
    "        return math.ceil(num)\n",
    "    else:\n",
    "        return math.floor(num)\n",
    "   \n",
    "def post_score(s):\n",
    "    with urlopen(\n",
    "    \"https://graph.facebook.com/v6.0/me?fields=id%2Cname%2Cposts%7Bmessage%2Ccreated_time%7D&access_token={}\".format(s)) as response:\n",
    "        source = response.read()\n",
    "       \n",
    "    data = json.loads(source)\n",
    "\n",
    "    some_data = data['posts']\n",
    "\n",
    "    outfile = open(\"post_data.csv\", \"w\")\n",
    "\n",
    "    fieldNames = ['created_time', 'id', 'message']\n",
    "\n",
    "    writer = csv.DictWriter(outfile, fieldnames = fieldNames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    while len(some_data['data']) != 0:\n",
    "        with open('post_data.json', 'a+', encoding='utf-8') as f:\n",
    "            json.dump(some_data['data'], f, ensure_ascii=False, indent=4)\n",
    "       \n",
    "        for row in some_data['data']:\n",
    "            writer.writerow(row)\n",
    "       \n",
    "        paging_next = some_data['paging']['next']\n",
    "       \n",
    "        with urlopen(paging_next) as response:\n",
    "            source = response.read()\n",
    "        some_data = json.loads(source)\n",
    "\n",
    "    df = pd.read_csv(\"post_data.csv\")\n",
    "\n",
    "    df = df[pd.notnull(df['message'])]\n",
    "\n",
    "    df.reset_index(drop = True, inplace=True)\n",
    "    df.to_csv(\"post_clean.csv\")#, index = False\n",
    "\n",
    "    senti_value = pd.Series([])\n",
    "    confidence = pd.Series([])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        temp = []\n",
    "        column = df['message'][i]\n",
    "        temp.append(column)\n",
    "        senti_dict = funcc(temp)\n",
    "        senti_value[i] = Senti_Extractor(senti_dict.get('compound'))\n",
    "        confidence[i] = senti_dict.get('compound')\n",
    "\n",
    "    df.insert(3, \"Senti_Value\", senti_value)\n",
    "    df.insert(4, \"Confidence\", confidence)\n",
    "\n",
    "    # Assigning weightage to DataTime\n",
    "\n",
    "    total_rows = len(df.index)\n",
    "    # total_rows = 21\n",
    "    even = False\n",
    "\n",
    "    time_weigh = pd.Series([])\n",
    "\n",
    "    if total_rows % 2 == 0:\n",
    "        even = True\n",
    "\n",
    "    if even == True:\n",
    "        i = total_rows/2 - 1\n",
    "        time_weigh[i] = 25\n",
    "        i = i-1\n",
    "        while(i >= 0):\n",
    "            time_weigh[i] = time_weigh[i+1]/2\n",
    "            i = i-1\n",
    "           \n",
    "        i = total_rows/2\n",
    "        time_weigh[i] = 25\n",
    "        i = i+1\n",
    "        while(i < total_rows):\n",
    "            time_weigh[i] = time_weigh[i-1]/2\n",
    "            i = i+1\n",
    "    else:\n",
    "        i = total_rows/2 - 0.5\n",
    "        time_weigh[i] = 50/3\n",
    "        time_weigh[i+1] = 50/3\n",
    "        time_weigh[i-1] = 50/3\n",
    "        time_weigh[i-2] = 25/2\n",
    "        i = i - 3\n",
    "        while (i >= 0):\n",
    "            time_weigh[i] = time_weigh[i+1]/2\n",
    "            i = i - 1\n",
    "           \n",
    "        i = total_rows/2 - 0.5\n",
    "        time_weigh[i+2] = 25/2\n",
    "        i = i+3\n",
    "        while (i < total_rows):\n",
    "            time_weigh[i] = time_weigh[i-1]/2\n",
    "            i = i+1\n",
    "           \n",
    "    df.insert(2, \"Time_Weight\", time_weigh)\n",
    "    df['Confi_Rank'] = df['Confidence'].rank(ascending = 0, method = 'dense')\n",
    "    # df.sort_values('Confi_Rank', inplace = True)\n",
    "    df = df.set_index('Confi_Rank')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    confi_weigh = pd.Series([])\n",
    "\n",
    "    confi_weigh[1] = 50\n",
    "\n",
    "    for ind in df.index:\n",
    "        if (ind != 1.0):\n",
    "            confi_weigh[ind] = confi_weigh[ind-1]/2\n",
    "\n",
    "    df.insert(6, \"Confi_Weight\", confi_weigh)\n",
    "\n",
    "    df['Overall'] = df['Time_Weight'] * df['Senti_Value'] * df['Confi_Weight']\n",
    "    df.to_csv('feed_complete.csv')\n",
    "    feed_score = df['Overall'].sum()\n",
    "\n",
    "    return feed_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494.95697021484375\n"
     ]
    }
   ],
   "source": [
    "user_short_token = 'EAAYCDwuzUQ4BAA0LyiBhQTQGuGaGnjbRESN202n5EyN2cmUFZAZAk00XNI6CGECcSsgucLmZClYHkHgutpUastMuzprrCBpJrjxY0DNONZBHO8LiPZCKVvxh9ZBNyfwM1ghxaoyvO3dmibnAgZB4NLAYxkpgMZCkojilFLR5AAZCgCzAX4j9yvZCZCPCuMLXaZAkRiIZD'\n",
    "print(post_score(user_short_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
